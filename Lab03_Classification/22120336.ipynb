{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Phân lớp\n",
    "\n",
    "- MSSV: 22120336\n",
    "- Họ và tên: Võ Tuấn Thành"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yêu cầu bài tập\n",
    "\n",
    "**Cách làm bài**\n",
    "\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; trong file, từ `TODO` để cho biết những phần mà bạn cần phải làm.\n",
    "\n",
    "Bạn có thể thảo luận ý tưởng cũng như tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn*. \n",
    "\n",
    "Nếu vi phạm thì sẽ bị 0 điểm cho bài tập này.\n",
    "\n",
    "**Cách nộp bài**\n",
    "\n",
    "Trước khi nộp bài, rerun lại notebook (`Kernel` -> `Restart & Run All`).\n",
    "\n",
    "Sau đó, đặt tên notebook bằng `MSSV` của bạn (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên notebook là `1234567.ipynb`) và nộp ở link trên moodle.\n",
    "\n",
    "**Nội dung bài tập**\n",
    "\n",
    "Trong bài này, bạn sẽ cài đặt 2 thuật toán phân lớp: \n",
    "1. Cây quyết định (Decision tree)\n",
    "2. Gaussian Naive Bayes\n",
    "\n",
    "**Một số lưu ý**\n",
    "1. Chỉ cần phát hiện có dấu hiện có sau chép code sẽ 0 điểm cả lab và có thể dẫn đến không điểm môn học\n",
    "2. Sai format nộp bài sẽ bị trừ 20% số điểm\n",
    "3. Trễ deadline 1 ngày sẽ bị trừ 40% số điểm. Sau 24h kể từ hạn nộp sẽ không nhận bất kì bài nộp bổ sung nào.\n",
    "4. Các thắc mắc về bài tập, các bạn vui lòng liên hệ giáo viên qua email `ntthuhang0131@gmail.com` với tiêu đề `[Cơ sở trí tuệ nhân tạo - CQ2022/22 - Thắc mắc lab 03]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thêm thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dữ liệu Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "iris=datasets.load_iris()\n",
    "\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "#split dataset into training data and testing data\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cây quyết định (Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Iterative Dichotomiser 3 (ID3) (2 đ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thông tin kỳ vọng (entropy):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Entropy=-\\sum_{i}^{n}p_ilog_{2}(p_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm entropy đạt giá trị nhỏ nhất nếu có một giá trị $p_i=1$, đạt giá trị lớn nhất nếu tất cả các $p_i$ bằng nhau. Những tính chất này của hàm entropy khiến nó được sử dụng trong việc đo độ hỗn loạn của một phép phân chia của ID3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(counts, n_samples):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    counts: shape (n_classes): list number of samples in each class\n",
    "    n_samples: number of data samples\n",
    "    \n",
    "    -----------\n",
    "    return entropy \n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    entropy = 0\n",
    "\n",
    "    for count in counts:\n",
    "        if count != 0:\n",
    "            entropy += -count/n_samples * np.log2(count/n_samples)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_of_one_division(division): \n",
    "    \"\"\"\n",
    "    Returns entropy of a divided group of data\n",
    "    Data may have multiple classes\n",
    "    \"\"\"\n",
    "    n_samples = len(division)\n",
    "    n_classes = set(division)\n",
    "    \n",
    "    counts=[]\n",
    "    #count samples in each class then store it to list counts\n",
    "    #TODO:\n",
    "    for i in n_classes:\n",
    "        counts.append(np.sum(division == i))\n",
    "    \n",
    "    return entropy(counts,n_samples),n_samples\n",
    "\n",
    "\n",
    "def get_entropy(y_predict, y):\n",
    "    \"\"\"\n",
    "    Returns entropy of a split\n",
    "    y_predict is the split decision by cutoff, True/Fasle\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    s_true, n_true = entropy_of_one_division(y[y_predict]) # left hand side entropy\n",
    "    s_false, n_false = entropy_of_one_division(y[~y_predict]) # right hand side entropy\n",
    "    s = n_true/n * s_true + n_false/n * s_false # overall entropy\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Độ lợi thông tin phân lớp tập D theo thuộc tính A:\n",
    "$$ Gain(A)=Entrophy(D)-Entrophy_{A}(D)$$\n",
    "\n",
    "Trong ID3, tại mỗi node, thuộc tính được chọn được xác định dựa trên là thuộc tính khiến cho information gain đạt giá trị lớn nhất.\n",
    "\n",
    "Các thuộc tính của tập Iris đều có giá trị liên tục. Do đó ta cần rời rạc hóa cho từng thuộc tính. Cách đơn giản là sử dụng một ngưỡng `cutoff` chia giá trị của dữ liệu trên mỗi thuộc tính sẽ làm 2 phần: `value<cutoff` và `value>=cutoff`.\n",
    "\n",
    "Để tìm ngưỡng `cutoff` tốt nhất cho mỗi thuộc tính ta lần lượt thay `cutoff` bằng các giá trị của thuộc tính sau đó tính entropy, `cutoff` tốt nhất khi entropy bé nhất.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, tree=None, depth=0):\n",
    "        '''Parameters:\n",
    "        -----------------\n",
    "        tree: decision tree\n",
    "        depth: depth of decision tree after training'''\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.tree=tree\n",
    "    def fit(self, X, y, node={}, depth=0):\n",
    "        '''Parameter:\n",
    "        -----------------\n",
    "        X: training data\n",
    "        y: label of training data\n",
    "        ------------------\n",
    "        return: node \n",
    "        \n",
    "        node: each node represented by cutoff value and column index, value and children.\n",
    "         - cutoff value is thresold where you divide your attribute\n",
    "         - column index is your data attribute index\n",
    "         - value of node is mean value of label indexes, \n",
    "           if a node is leaf all data samples will have same label\n",
    "        \n",
    "        Note that: we divide each attribute into 2 part => each node will have 2 children: left, right.\n",
    "        '''\n",
    "        \n",
    "        #Stop conditions\n",
    "        \n",
    "        #if all value of y are the same \n",
    "        if np.all(y==y[0]):\n",
    "            return {'val':y[0]}\n",
    "\n",
    "        else: \n",
    "            col_idx, cutoff, entropy = self.find_best_split_of_all(X, y)    # find one split given an information gain \n",
    "            y_left = y[X[:, col_idx] < cutoff]\n",
    "            y_right = y[X[:, col_idx] >= cutoff]\n",
    "            node = {'index_col':col_idx,\n",
    "                        'cutoff':cutoff,\n",
    "                   'val':np.mean(y)}\n",
    "            node['left'] = self.fit(X[X[:, col_idx] < cutoff], y_left, {}, depth+1)\n",
    "            node['right'] = self.fit(X[X[:, col_idx] >= cutoff], y_right, {}, depth+1)\n",
    "            self.depth += 1 \n",
    "            self.tree = node\n",
    "            return node\n",
    "    \n",
    "    def find_best_split_of_all(self, X, y):\n",
    "        col_idx = None\n",
    "        min_entropy = 1\n",
    "        cutoff = None\n",
    "        for i, col_data in enumerate(X.T):\n",
    "            entropy, cur_cutoff = self.find_best_split(col_data, y)\n",
    "            if entropy == 0:                   #best entropy\n",
    "                return i, cur_cutoff, entropy\n",
    "            elif entropy <= min_entropy:\n",
    "                min_entropy = entropy\n",
    "                col_idx = i\n",
    "                cutoff = cur_cutoff\n",
    "               \n",
    "        return col_idx, cutoff, min_entropy\n",
    "    \n",
    "    def find_best_split(self, col_data, y):\n",
    "        ''' Parameters:\n",
    "        -------------\n",
    "        col_data: data samples in column'''\n",
    "         \n",
    "        min_entropy = 10\n",
    "        cutoff = None\n",
    "\n",
    "        #Loop through col_data find cutoff where entropy is minimum\n",
    "        #TODO\n",
    "        for value in np.unique(col_data):\n",
    "            y_predict = col_data < value\n",
    "            entropy = get_entropy(y_predict, y)\n",
    "            \n",
    "            if entropy == 0:  # best entropy\n",
    "                return entropy, value\n",
    "            elif entropy <= min_entropy:\n",
    "                min_entropy = entropy\n",
    "                cutoff = value\n",
    "    \n",
    "        return min_entropy, cutoff\n",
    "                                               \n",
    "    def predict(self, X):\n",
    "        tree = self.tree\n",
    "        pred = np.zeros(shape=len(X))\n",
    "        for i, c in enumerate(X):\n",
    "            pred[i] = self._predict(c)\n",
    "        return pred\n",
    "    \n",
    "    def _predict(self, row):\n",
    "        cur_layer = self.tree\n",
    "        while cur_layer.get('cutoff'):\n",
    "            if row[cur_layer['index_col']] < cur_layer['cutoff']:\n",
    "                cur_layer = cur_layer['left']\n",
    "            else:\n",
    "                cur_layer = cur_layer['right']\n",
    "        else:\n",
    "            return cur_layer.get('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Classification on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of your decision tree model on training data: 1.0\n",
      "Accuracy of your decision tree model: 0.96\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "tree = model.fit(X_train, y_train)\n",
    "pred=model.predict(X_train)\n",
    "print('Accuracy of your decision tree model on training data:', accuracy_score(y_train,pred))\n",
    "pred=model.predict(X_test)\n",
    "print('Accuracy of your decision tree model:', accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 CART (Classification and Regression Trees) (2 đ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các bạn tự cài đặt lại thuật toán CART và test trên tập dữ liệu Iris (tham khảo cách trình bày ở `phần 1.1`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Index\n",
    "\n",
    "- Công thức:\n",
    "$$Gini(D)=1-\\sum_{i=1}^{n}p_i^2$$\n",
    "- Gini index càng nhỏ thì độ tinh khiết của tập dữ liệu càng cao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(counts, n_samples):\n",
    "    res = 1\n",
    "    for count in counts:\n",
    "        if count == 0: continue\n",
    "        pi = count / n_samples\n",
    "        res -= pi ** 2\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Tree node for CART decision tree\"\"\"\n",
    "    def __init__(self, predicted_class=None, threshold=None, feature_index=None, \n",
    "                 left=None, right=None, gini=None, n_samples=None):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.threshold = threshold\n",
    "        self.feature_index = feature_index\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.gini = gini\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "class DecisionTreeClassifierWithGini:\n",
    "    \"\"\"CART Decision Tree Classifier using Gini impurity\"\"\"\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "        self.n_classes = None\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"Find the best split for a node\"\"\"\n",
    "        best_gini = float('inf')\n",
    "        best_threshold = None\n",
    "        best_feature = None\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        # If pure node, return immediately\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return None, None, gini_index(np.bincount(y, minlength=self.n_classes), n_samples)\n",
    "            \n",
    "        # Try each feature\n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            # Get unique values for threshold testing\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            \n",
    "            # Try each threshold\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                \n",
    "                # Skip if split would result in empty node\n",
    "                if not 0 < left_mask.sum() < n_samples:\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate gini impurity for split\n",
    "                left_counts = np.bincount(y[left_mask], minlength=self.n_classes)\n",
    "                right_counts = np.bincount(y[~left_mask], minlength=self.n_classes)\n",
    "                \n",
    "                left_gini = gini_index(left_counts, left_mask.sum())\n",
    "                right_gini = gini_index(right_counts, (~left_mask).sum())\n",
    "                \n",
    "                # Weighted average of gini values\n",
    "                gini = (left_mask.sum() * left_gini + (~left_mask).sum() * right_gini) / n_samples\n",
    "                \n",
    "                # Update best split if this split is better\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_threshold = threshold\n",
    "                    best_feature = feature_idx\n",
    "                    \n",
    "        return best_feature, best_threshold, best_gini\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"Recursively build the decision tree\"\"\"\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        # Count samples per class\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        \n",
    "        # Check stopping criteria\n",
    "        if ((self.max_depth is not None and depth >= self.max_depth) or \n",
    "            n_samples < self.min_samples_split or \n",
    "            len(classes) == 1):\n",
    "            return Node(\n",
    "                predicted_class=classes[np.argmax(counts)],\n",
    "                gini=gini_index(np.bincount(y, minlength=self.n_classes), n_samples),\n",
    "                n_samples=n_samples\n",
    "            )\n",
    "        \n",
    "        # Find best split\n",
    "        feature_idx, threshold, gini = self._best_split(X, y)\n",
    "        \n",
    "        # If no valid split found, make leaf node\n",
    "        if feature_idx is None:\n",
    "            return Node(\n",
    "                predicted_class=classes[np.argmax(counts)],\n",
    "                gini=gini,\n",
    "                n_samples=n_samples\n",
    "            )\n",
    "        \n",
    "        # Split data\n",
    "        left_mask = X[:, feature_idx] <= threshold\n",
    "        \n",
    "        # Create decision node\n",
    "        node = Node(\n",
    "            feature_index=feature_idx,\n",
    "            threshold=threshold,\n",
    "            gini=gini,\n",
    "            n_samples=n_samples\n",
    "        )\n",
    "        \n",
    "        # Recursively build children\n",
    "        node.left = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        node.right = self._build_tree(X[~left_mask], y[~left_mask], depth + 1)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit decision tree to training data\"\"\"\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.root = self._build_tree(X, y)\n",
    "        return self\n",
    "    \n",
    "    def _predict_single(self, x, node):\n",
    "        \"\"\"Predict class for a single sample\"\"\"\n",
    "        if node.predicted_class is not None:\n",
    "            return node.predicted_class\n",
    "            \n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._predict_single(x, node.left)\n",
    "        return self._predict_single(x, node.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for multiple samples\"\"\"\n",
    "        return np.array([self._predict_single(x, self.root) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Classification on Iris Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of your decision tree model on training data: 0.97\n",
      "Accuracy of your decision tree model on test data: 0.98\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifierWithGini(max_depth=3)\n",
    "tree = model.fit(X_train, y_train)\n",
    "\n",
    "pred_train = model.predict(X_train)\n",
    "print('Accuracy of your decision tree model on training data:', accuracy_score(y_train, pred_train))\n",
    "\n",
    "pred_test = model.predict(X_test)\n",
    "print('Accuracy of your decision tree model on test data:', accuracy_score(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Câu hỏi lý thuyết (2 đ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Đây là câu hỏi về lý thuyết có thể giúp các bạn buổi sung kiến thức cho các buổi phỏng vấn. Các bạn có thể tham khảo bất kì nguồn nào trừ ChatGPT và có thể thêm hình ảnh minh họa để có câu trả lời rõ ràng và dễ hiểu nhất. Và khi đã tham khảo phải liệt kê tài liệu tham khảo? Trường hợp phát hiện đạo văn bài làm sẽ nhận 0 điểm ngay lập tức.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. So sánh Entropy, Gini Impurity và Variance Reduction trong Decision Tree. Khi nào nên sử dụng mỗi tiêu chí?\n",
    "\n",
    "Câu trả lời:\n",
    "\n",
    "Trong Decision Tree, Entropy, Gini Impurity và Variance Reduction là các tiêu chí được sử dụng để đánh giá độ \"tinh khiết\" của một tập dữ liệu (node) và chọn thuộc tính tốt nhất để phân chia dữ liệu tại mỗi node. Mục tiêu là giảm độ \"bất ổn\" (impurity) hoặc \"không chắc chắn\" (uncertainty) trong các node con sau khi phân chia.\n",
    "\n",
    "*   **Entropy và Gini Impurity:** Dùng cho bài toán phân loại (classification). Entropy thường được ưu tiên khi muốn tính toán độ \"bất ổn\" chính xác, Gini Impurity khi ưu tiên hiệu suất tính toán.\n",
    "*   **Variance Reduction:** Dùng cho bài toán hồi quy (regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Overfitting trong Decision Tree là gì? Bạn có thể làm gì để giảm thiểu vấn đề này?\n",
    "Câu trả lời:\n",
    " -   Overfitting xảy ra khi Decision Tree được huấn luyện quá kỹ trên dữ liệu huấn luyện, dẫn đến việc nó ghi nhớ các chi tiết nhiễu (noise) hoặc các trường hợp ngoại lệ trong dữ liệu huấn luyện thay vì học được các quy luật tổng quát.\n",
    " -   Hậu quả là cây quyết định hoạt động rất tốt trên dữ liệu huấn luyện, nhưng lại hoạt động kém khi được áp dụng lên dữ liệu mới, chưa từng thấy (dữ liệu kiểm tra).\n",
    " -   Biểu hiện của overfitting trong Decision Tree thường là một cây rất sâu, có nhiều node lá, và có nhiều nhánh chỉ xử lý các trường hợp đặc biệt trong dữ liệu huấn luyện."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Decision Tree có phù hợp cho dữ liệu nhiều chiều hoặc dữ liệu không cân bằng không? Tại sao? \n",
    "Câu trả lời:\n",
    "\n",
    "Descision Tree không lý tưởng cho dữ liệu nhiều chiều hoặc dữ liệu không cân bằng. Vì:\n",
    "*   **Dữ liệu nhiều chiều (High-dimensional data):**\n",
    "    *   **Không lý tưởng:** Decision Tree có thể gặp khó khăn do \"Curse of dimensionality\", dẫn đến overfitting và tốn nhiều tài nguyên. Cây có thể trở nên quá phức tạp, ghi nhớ chi tiết nhiễu thay vì học quy luật chung.\n",
    "    *   **Lý do:** Số lượng thuộc tính lớn làm tăng số nhánh và node, khiến việc chọn thuộc tính phân chia tốt trở nên khó khăn.\n",
    "    *   **Cần:** Các biện pháp giảm chiều dữ liệu, kỹ thuật điều chuẩn hoặc sử dụng các mô hình ensemble (như Random Forest).\n",
    "*   **Dữ liệu không cân bằng (Imbalanced data):**\n",
    "    *   **Không lý tưởng:** Decision Tree có xu hướng nghiêng về lớp đa số, bỏ qua các lớp thiểu số quan trọng.\n",
    "    *   **Lý do:** Cây được tối ưu để giảm lỗi tổng thể, và dễ dàng đạt được điều đó bằng cách dự đoán đúng cho lớp đa số.\n",
    "    *   **Cần:** Các kỹ thuật như trọng số mẫu, upsampling, downsampling, SMOTE, cost-sensitive learning hoặc sử dụng các mô hình ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Nếu bạn có một Decision Tree quá lớn (deep tree), bạn sẽ làm gì để cải thiện khả năng tổng quát của nó?\n",
    "Câu trả lời:\n",
    "\n",
    "Khi một Decision Tree quá lớn (deep tree), nó thường bị overfitting, tức là hoạt động rất tốt trên dữ liệu huấn luyện nhưng kém trên dữ liệu mới. Để cải thiện khả năng tổng quát của nó, có thể thực hiện các biện pháp sau, tương tự như đã đề cập trong câu 2:\n",
    "\n",
    "1.  **Giới hạn chiều sâu của cây (Maximum Depth):**\n",
    "    *   Sử dụng tham số `max_depth` để giới hạn số lượng level tối đa của cây. Điều này ngăn cây không đi quá sâu và bắt đầu ghi nhớ dữ liệu huấn luyện một cách cụ thể.\n",
    "    *   Bạn có thể tìm giá trị `max_depth` phù hợp bằng cách sử dụng kỹ thuật cross-validation và theo dõi hiệu suất của mô hình trên dữ liệu kiểm tra.\n",
    "2.  **Tăng số lượng mẫu tối thiểu trên một node (Minimum Samples Split/Leaf):**\n",
    "    *   Sử dụng tham số `min_samples_split` để yêu cầu một số lượng mẫu tối thiểu để node được chia tiếp.\n",
    "    *   Sử dụng tham số `min_samples_leaf` để yêu cầu một số lượng mẫu tối thiểu trong mỗi node lá.\n",
    "    *   Các tham số này giúp ngăn cây tạo ra các node lá quá nhỏ hoặc không đáng tin cậy.\n",
    "3.  **Tỉa cây (Pruning):**\n",
    "    *   **Pre-pruning:** Áp dụng các ràng buộc ngay từ đầu bằng cách sử dụng `max_depth`, `min_samples_split`, `min_samples_leaf`,...\n",
    "    *   **Post-pruning:** Cho phép cây phát triển hết, sau đó loại bỏ các nhánh không quan trọng bằng cách sử dụng các thuật toán tỉa cây như Reduced Error Pruning hoặc Cost-Complexity Pruning.\n",
    "4.  **Sử dụng Cross-Validation:**\n",
    "    *   Chia dữ liệu thành nhiều phần (ví dụ: k-fold cross-validation).\n",
    "    *   Huấn luyện mô hình trên các tập dữ liệu huấn luyện khác nhau và đánh giá hiệu suất trên tập dữ liệu kiểm tra tương ứng.\n",
    "    *   Sử dụng hiệu suất trung bình trên các tập kiểm tra để đánh giá hiệu quả của mô hình và chọn các giá trị tham số tốt nhất.\n",
    "5.  **Sử dụng Ensemble Methods:**\n",
    "    *   Thay vì một Decision Tree, hãy sử dụng các phương pháp ensemble như Random Forest hoặc Gradient Boosting.\n",
    "    *   Các mô hình ensemble kết hợp kết quả dự đoán của nhiều Decision Tree khác nhau, thường cho kết quả tốt hơn và ít bị overfitting hơn so với một Decision Tree đơn lẻ.\n",
    "6.  **Lựa chọn thuộc tính (Feature Selection) và tạo thuộc tính (Feature Engineering):**\n",
    "    *   Kiểm tra xem có thuộc tính nào không đóng góp nhiều vào kết quả và loại bỏ chúng.\n",
    "    *   Có thể tạo thêm thuộc tính mới từ các thuộc tính hiện có để cung cấp thông tin có ích hơn cho mô hình.\n",
    "7.  **Thử nghiệm với nhiều tham số khác nhau:**\n",
    "    *   Sử dụng Grid Search hoặc Random Search kết hợp với Cross-Validation để tìm ra tổ hợp các giá trị tham số tốt nhất cho mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tài liệu tham khảo:\n",
    "*   Géron, A. (2019). *Hands-on machine learning with Scikit-Learn, Keras & TensorFlow: Concepts, tools, and techniques to build intelligent systems*. O'Reilly Media. (Chapter 6: Decision Trees)\n",
    "*   Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer Science & Business Media. (Chapter 9: Additive Models, Trees, and Related Methods)\n",
    "*   w3schools. (n.d.). Python Machine Learning - Decision Tree. Retrieved from https://www.w3schools.com/python/python_ml_decision_tree.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Định lý Bayes (4 đ)\n",
    "\n",
    "Định lý Bayes được phát biểu dưới dạng toán học như sau:\n",
    "$$\\begin{equation}\n",
    "P\\left(A|B\\right)= \\dfrac{P\\left(B|A\\right)P\\left(A\\right)}{P\\left(B\\right)}\n",
    "\\end{equation}$$\n",
    "\n",
    "Nếu ta coi $B$ là dữ liệu $\\mathcal{D}$, các thông số cần ước tính $A$ là $w$, ta có:\n",
    "\n",
    "$$ \\begin{align}\n",
    "    \\underbrace{P(w|\\mathcal{D})}_{Posterior}= \\dfrac{1}{\\underbrace{P(\\mathcal{D})}_{Normalization}} \\overbrace{P(\\mathcal{D}|w)}^{\\text{Likelihood}} \\overbrace{P(w)}^{Prior}\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "Để giúp cho việc tính toán được đơn giản, người ta thường giả sử một cách đơn giản nhất rằng các thành phần của biến ngẫu nhiên $D$ (hay các thuộc tính của dữ liệu $D$) là độc lập với nhau, nếu biết $w$. Tức là:\n",
    "$$P(\\mathcal{D}|w)=\\prod _{i=1}^{d}P(x_i|w)$$\n",
    "\n",
    "$d$: số lượng thuộc tính\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Probability Density Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pdf:\n",
    "    def __init__(self,hist=None):\n",
    "        '''\n",
    "        A probability density function represented by a histogram\n",
    "        \n",
    "        hist: shape (n,1), n: number of hypotheses\n",
    "        hypo: hypothesis (simply understand as label)\n",
    "        hist[hypo]=P(hypo)\n",
    "        '''\n",
    "        self.hist = hist\n",
    "        \n",
    "    #virtual function\n",
    "    def likelihood(self, data, hypo):\n",
    "        '''Paramters:\n",
    "        data: new data record \n",
    "        hypo: hypothesis (simply understand as label)\n",
    "        ---------\n",
    "        return P(data/hypo)\n",
    "        ''' \n",
    "        raise Exception()\n",
    "            \n",
    "    #update histogram for new data \n",
    "    def update(self, data):\n",
    "        ''' \n",
    "        P(hypo/data)=P(data/hypo)*P(hypo)*(1/P(data))\n",
    "        '''\n",
    "        \n",
    "        #Likelihood * Prior \n",
    "        #TODO\n",
    "        for hypo in self.hist.keys():\n",
    "            self.hist[hypo] *= self.likelihood(data, hypo)\n",
    "       \n",
    "            \n",
    "        #Normalization\n",
    "        \n",
    "        #TODO: s=P(data)\n",
    "        #s=?\n",
    "        s = 0\n",
    "        for hypo in self.hist.keys():\n",
    "            s += self.hist[hypo]\n",
    "\n",
    "        for hypo in self.hist.keys():\n",
    "            self.hist[hypo] = self.hist[hypo]/s\n",
    "        \n",
    "    def plot_pdf(self):\n",
    "        #plot Histogram\n",
    "        #TODO\n",
    "        plt.bar(self.hist.keys(), self.hist.values())\n",
    "        plt.show()\n",
    "      \n",
    "    \n",
    "    def maxHypo(self):\n",
    "        #find the hypothesis with maximum probability from hist\n",
    "        #TODO\n",
    "        return max(self.hist, key=self.hist.get)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Classification on Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes có thể được mở rộng cho dữ liệu với các thuộc tính có giá trị là số thực, phổ biến nhất bằng cách sử dụng phân phối chuẩn (Gaussian distribution).\n",
    "\n",
    "- Phần mở rộng này được gọi là Gaussian Naive Bayes. Các hàm khác có thể được sử dụng để ước tính phân phối dữ liệu, nhưng Gaussian (hoặc phân phối chuẩn) là dễ nhất để làm việc vì chỉ cần ước tính giá trị trung bình và độ lệch chuẩn từ dữ liệu huấn luyện."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Định nghĩa hàm Gauss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f\\left(x;\\mu,\\sigma \\right)= \\dfrac{1}{\\sigma \\sqrt{2\\pi}} \n",
    "\\exp \\left({-\\dfrac{\\left(x-\\mu\\right)^2}{2 \\sigma^2}}\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gauss(std,mean,x):\n",
    "    #Calculate the Gaussian probability distribution function for x\n",
    "    #TODO \n",
    "    exp = -((x - mean) ** 2) / (2 * std ** 2)\n",
    "    return (1 / (std * np.sqrt(2 * np.pi))) * np.exp(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBGaussian(pdf):\n",
    "    def __init__(self, hist=None, std=None, mean=None):\n",
    "        '''Parameters:\n",
    "        \n",
    "        '''\n",
    "        pdf.__init__(self, hist)\n",
    "        self.std=std\n",
    "        self.mean=mean\n",
    "    def likelihood(self,data, hypo):\n",
    "        '''\n",
    "        Returns: P(data/hypo)\n",
    "        -----------------\n",
    "        Naive bayes:\n",
    "            Atributes are assumed to be conditionally independent given the class value.\n",
    "        '''\n",
    "    \n",
    "        std=self.std[hypo]\n",
    "        mean=self.mean[hypo]\n",
    "        res=1\n",
    "        #TODO\n",
    "        #res=res*P(xi/hypo)\n",
    "        for i in range(len(data)):\n",
    "            res *= Gauss(std[i], mean[i], data[i])\n",
    "            \n",
    "        return res \n",
    "    def fit(self, X,y):\n",
    "        \"\"\"Parameters:\n",
    "        X: training data\n",
    "        y: labels of training data\n",
    "        \"\"\"\n",
    "        n=len(X)\n",
    "        #number of iris species\n",
    "        #TODO\n",
    "        n_species = len(np.unique(y))\n",
    "        \n",
    "        hist={}\n",
    "        mean={}\n",
    "        std={}\n",
    "        \n",
    "        #separate  dataset into rows by class\n",
    "        for hypo in range(0,n_species):\n",
    "            #TODO rows=?\n",
    "            rows = np.count_nonzero(y == hypo)\n",
    "            \n",
    "            #histogram for each hypo\n",
    "            #TODO probability=?\n",
    "            probability = rows / n\n",
    "            hist[hypo] = probability\n",
    "            \n",
    "            #Gaussian naive bayes each hypothesis represented by its mean and standard derivation\n",
    "            '''mean and standard derivation should be calculated for each column (or each attribute)'''\n",
    "            #TODO mean[hypo]=?, std[hypo]=?\n",
    "            mean[hypo] = np.mean(X[y == hypo], axis=0)\n",
    "            std[hypo] = np.std(X[y == hypo], axis=0)\n",
    "         \n",
    "        self.mean=mean\n",
    "        self.std=std\n",
    "        self.hist=hist\n",
    "   \n",
    "    def _predict(self, data, plot=False):\n",
    "        \"\"\"\n",
    "        Predict label for only 1 data record\n",
    "        ------------\n",
    "        Parameters:\n",
    "        data: new data record\n",
    "        plot: True: draw histogram after update new record\n",
    "        -----------\n",
    "        return: label of data\n",
    "        \"\"\"\n",
    "        model=NBGaussian(hist=self.hist.copy(),std=self.std.copy(), mean=self.mean.copy())\n",
    "        model.update(data)\n",
    "        if (plot): model.plot_pdf()\n",
    "        return model.maxHypo()\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"Parameters:\n",
    "        Data: test data\n",
    "        ----------\n",
    "        return labels of test data\"\"\"\n",
    "        \n",
    "        pred=[]\n",
    "        for x in data:\n",
    "            pred.append(self._predict(x))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vẽ histogram of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_1=NBGaussian()\n",
    "model_1.fit(X_train, y_train)\n",
    "model_1.plot_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhận xét chart:\n",
    "- Các histogram có dạng hình chuông, đối xứng qua trục tung, và có đỉnh tại giá trị trung bình của dữ liệu.\n",
    "- Các histogram có độ rộng khác nhau, phản ánh sự khác biệt về độ biến thiên của dữ liệu giữa các lớp.\n",
    "- Các histogram có đỉnh cao hơn ở các lớp 0 và 1, phản ánh sự phân bố dữ liệu tập trung hơn ở các lớp này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of X_test[10]:  2\n",
      "Our histogram after update X_test[10]: \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHihJREFUeJzt3X901fV9+PFXEk2iRxNklARotvijRVstUJAsuFY9Tc1aDht/7IyhE8bxx/TQHjRb26RVMutmtKci58x0rK7UnXYe6Y9pdwrDQ6PUY01lBjhHLdrhL6g1AcaaYOxIm3y+f/j12ghBbkLIm/B4nHP/4MP7fe/7vs899z7P5/5IQZZlWQAAJKRwrBcAAPBuAgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDknDLWCzgaAwMD8ctf/jLOPPPMKCgoGOvlAABHIcuyOHDgQEydOjUKC/M7J3JCBMovf/nLqKqqGutlAADDsHv37nj/+9+f15wTIlDOPPPMiHjrDpaVlY3xagCAo9HT0xNVVVW51/F8nBCB8vbbOmVlZQIFAE4ww/l4hg/JAgDJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcvIOlMcffzwWLFgQU6dOjYKCgnj44Yffc87mzZvjox/9aJSUlMR5550X999//zCWCgCcLPIOlN7e3pgxY0a0trYe1fiXX3455s+fH5dffnls3749brrpprj22mvjkUceyXuxAMDJIe8/FvipT30qPvWpTx31+DVr1sTZZ58dd999d0REXHDBBfHEE0/EPffcE/X19fnePABwEhj1z6C0t7dHXV3doGP19fXR3t4+5JyDBw9GT0/PoAsAcPLI+wxKvjo7O6OiomLQsYqKiujp6Ylf//rXcdpppx0yp6WlJW677bbRXhoAEVHduH6sl8AYe+XO+WO9hEMk+S2epqam6O7uzl1279491ksCAI6jUT+DUllZGV1dXYOOdXV1RVlZ2WHPnkRElJSURElJyWgvDQBI1KifQamtrY22trZBxzZt2hS1tbWjfdMAwAkq70B54403Yvv27bF9+/aIeOtrxNu3b49du3ZFxFtvzyxZsiQ3/oYbboiXXnopPv/5z8fzzz8fX/va1+I73/lO3HzzzcfmHgAA407egfL000/HrFmzYtasWRER0dDQELNmzYqVK1dGRMTrr7+ei5WIiLPPPjvWr18fmzZtihkzZsTdd98d//Iv/+IrxgDAkAqyLMvGehHvpaenJ8rLy6O7uzvKysrGejkA44pv8TBa3+IZyet3kt/iAQBObgIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSM6xAaW1tjerq6igtLY2amprYsmXLEcevXr06pk+fHqeddlpUVVXFzTffHP/3f/83rAUDAONf3oGybt26aGhoiObm5ti6dWvMmDEj6uvrY8+ePYcd/8ADD0RjY2M0NzfHjh074hvf+EasW7cuvvjFL4548QDA+JR3oKxatSquu+66WLZsWXzoQx+KNWvWxOmnnx5r16497Pgnn3wyLrnkkrjyyiujuro6rrjiili8ePF7nnUBAE5eeQVKX19fdHR0RF1d3TtXUFgYdXV10d7eftg58+bNi46OjlyQvPTSS7Fhw4b49Kc/PYJlAwDj2Sn5DN63b1/09/dHRUXFoOMVFRXx/PPPH3bOlVdeGfv27Ys/+qM/iizL4re//W3ccMMNR3yL5+DBg3Hw4MHcv3t6evJZJgBwghv1b/Fs3rw57rjjjvja174WW7dujX//93+P9evXx+233z7knJaWligvL89dqqqqRnuZAEBC8jqDMmnSpCgqKoqurq5Bx7u6uqKysvKwc2699da4+uqr49prr42IiIsuuih6e3vj+uuvjy996UtRWHhoIzU1NUVDQ0Pu3z09PSIFAE4ieZ1BKS4ujtmzZ0dbW1vu2MDAQLS1tUVtbe1h57z55puHREhRUVFERGRZdtg5JSUlUVZWNugCAJw88jqDEhHR0NAQS5cujTlz5sTcuXNj9erV0dvbG8uWLYuIiCVLlsS0adOipaUlIiIWLFgQq1atilmzZkVNTU3s3Lkzbr311liwYEEuVAAAflfegbJo0aLYu3dvrFy5Mjo7O2PmzJmxcePG3Adnd+3aNeiMyS233BIFBQVxyy23xGuvvRbve9/7YsGCBfEP//APx+5eAADjSkE21PssCenp6Yny8vLo7u72dg/AMVbduH6sl8AYe+XO+aNyvSN5/fa3eACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIzrEBpbW2N6urqKC0tjZqamtiyZcsRx//qV7+K5cuXx5QpU6KkpCQ++MEPxoYNG4a1YABg/Dsl3wnr1q2LhoaGWLNmTdTU1MTq1aujvr4+XnjhhZg8efIh4/v6+uKTn/xkTJ48Ob73ve/FtGnT4tVXX40JEyYci/UDAONQ3oGyatWquO6662LZsmUREbFmzZpYv359rF27NhobGw8Zv3bt2ti/f388+eSTceqpp0ZERHV19chWDQCMa3m9xdPX1xcdHR1RV1f3zhUUFkZdXV20t7cfds5//Md/RG1tbSxfvjwqKiriwgsvjDvuuCP6+/uHvJ2DBw9GT0/PoAsAcPLIK1D27dsX/f39UVFRMeh4RUVFdHZ2HnbOSy+9FN/73veiv78/NmzYELfeemvcfffd8fd///dD3k5LS0uUl5fnLlVVVfksEwA4wY36t3gGBgZi8uTJ8fWvfz1mz54dixYtii996UuxZs2aIec0NTVFd3d37rJ79+7RXiYAkJC8PoMyadKkKCoqiq6urkHHu7q6orKy8rBzpkyZEqeeemoUFRXljl1wwQXR2dkZfX19UVxcfMickpKSKCkpyWdpAMA4ktcZlOLi4pg9e3a0tbXljg0MDERbW1vU1tYeds4ll1wSO3fujIGBgdyxn//85zFlypTDxgkAQN5v8TQ0NMR9990X//qv/xo7duyIG2+8MXp7e3Pf6lmyZEk0NTXlxt94442xf//+WLFiRfz85z+P9evXxx133BHLly8/dvcCABhX8v6a8aJFi2Lv3r2xcuXK6OzsjJkzZ8bGjRtzH5zdtWtXFBa+0z1VVVXxyCOPxM033xwf+chHYtq0abFixYr4whe+cOzuBQAwrhRkWZaN9SLeS09PT5SXl0d3d3eUlZWN9XIAxpXqxvVjvQTG2Ct3zh+V6x3J67e/xQMAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcoYVKK2trVFdXR2lpaVRU1MTW7ZsOap5Dz74YBQUFMTChQuHc7MAwEki70BZt25dNDQ0RHNzc2zdujVmzJgR9fX1sWfPniPOe+WVV+Jv//Zv42Mf+9iwFwsAnBzyDpRVq1bFddddF8uWLYsPfehDsWbNmjj99NNj7dq1Q87p7++Pq666Km677bY455xzRrRgAGD8yytQ+vr6oqOjI+rq6t65gsLCqKuri/b29iHnffnLX47JkyfHNddcc1S3c/Dgwejp6Rl0AQBOHnkFyr59+6K/vz8qKioGHa+oqIjOzs7DznniiSfiG9/4Rtx3331HfTstLS1RXl6eu1RVVeWzTADgBDeq3+I5cOBAXH311XHffffFpEmTjnpeU1NTdHd35y67d+8exVUCAKk5JZ/BkyZNiqKioujq6hp0vKurKyorKw8Z/+KLL8Yrr7wSCxYsyB0bGBh464ZPOSVeeOGFOPfccw+ZV1JSEiUlJfksDQAYR/I6g1JcXByzZ8+Otra23LGBgYFoa2uL2traQ8aff/758cwzz8T27dtzlz/5kz+Jyy+/PLZv3+6tGwDgsPI6gxIR0dDQEEuXLo05c+bE3LlzY/Xq1dHb2xvLli2LiIglS5bEtGnToqWlJUpLS+PCCy8cNH/ChAkREYccBwB4W96BsmjRoti7d2+sXLkyOjs7Y+bMmbFx48bcB2d37doVhYV+oBYAGL6CLMuysV7Ee+np6Yny8vLo7u6OsrKysV4OwLhS3bh+rJfAGHvlzvmjcr0jef12qgMASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5wwqU1tbWqK6ujtLS0qipqYktW7YMOfa+++6Lj33sY3HWWWfFWWedFXV1dUccDwCQd6CsW7cuGhoaorm5ObZu3RozZsyI+vr62LNnz2HHb968ORYvXhyPPfZYtLe3R1VVVVxxxRXx2muvjXjxAMD4VJBlWZbPhJqamrj44ovj3nvvjYiIgYGBqKqqis9+9rPR2Nj4nvP7+/vjrLPOinvvvTeWLFlyVLfZ09MT5eXl0d3dHWVlZfksF4D3UN24fqyXwBh75c75o3K9I3n9zusMSl9fX3R0dERdXd07V1BYGHV1ddHe3n5U1/Hmm2/Gb37zm5g4ceKQYw4ePBg9PT2DLgDAySOvQNm3b1/09/dHRUXFoOMVFRXR2dl5VNfxhS98IaZOnTooct6tpaUlysvLc5eqqqp8lgkAnOCO67d47rzzznjwwQfjoYceitLS0iHHNTU1RXd3d+6ye/fu47hKAGCsnZLP4EmTJkVRUVF0dXUNOt7V1RWVlZVHnPvVr3417rzzzvjRj34UH/nIR444tqSkJEpKSvJZGgAwjuR1BqW4uDhmz54dbW1tuWMDAwPR1tYWtbW1Q877yle+Erfffnts3Lgx5syZM/zVAgAnhbzOoERENDQ0xNKlS2POnDkxd+7cWL16dfT29sayZcsiImLJkiUxbdq0aGlpiYiIu+66K1auXBkPPPBAVFdX5z6rcsYZZ8QZZ5xxDO8KADBe5B0oixYtir1798bKlSujs7MzZs6cGRs3bsx9cHbXrl1RWPjOiZl/+qd/ir6+vvizP/uzQdfT3Nwcf/d3fzey1QMA41Lev4MyFvwOCsDo8TsonPC/gwIAcDwIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASM6wAqW1tTWqq6ujtLQ0ampqYsuWLUcc/93vfjfOP//8KC0tjYsuuig2bNgwrMUCACeHvANl3bp10dDQEM3NzbF169aYMWNG1NfXx549ew47/sknn4zFixfHNddcE9u2bYuFCxfGwoUL49lnnx3x4gGA8akgy7Isnwk1NTVx8cUXx7333hsREQMDA1FVVRWf/exno7Gx8ZDxixYtit7e3vjhD3+YO/aHf/iHMXPmzFizZs1R3WZPT0+Ul5dHd3d3lJWV5bNcAN5DdeP6sV4CY+yVO+ePyvWO5PX7lHwG9/X1RUdHRzQ1NeWOFRYWRl1dXbS3tx92Tnt7ezQ0NAw6Vl9fHw8//PCQt3Pw4ME4ePBg7t/d3d0R8dYdBeDYGjj45lgvgTE2Wq+vb19vnudCIiLPQNm3b1/09/dHRUXFoOMVFRXx/PPPH3ZOZ2fnYcd3dnYOeTstLS1x2223HXK8qqoqn+UCAEehfPXoXv+BAweivLw8rzl5Bcrx0tTUNOisy8DAQOzfvz9+7/d+LwoKCnLHe3p6oqqqKnbv3u2tn2GyhyNj/0bOHo6M/Rs5ezgyR9q/LMviwIEDMXXq1LyvN69AmTRpUhQVFUVXV9eg411dXVFZWXnYOZWVlXmNj4goKSmJkpKSQccmTJgw5PiysjIPqhGyhyNj/0bOHo6M/Rs5ezgyQ+1fvmdO3pbXt3iKi4tj9uzZ0dbWljs2MDAQbW1tUVtbe9g5tbW1g8ZHRGzatGnI8QAAeb/F09DQEEuXLo05c+bE3LlzY/Xq1dHb2xvLli2LiIglS5bEtGnToqWlJSIiVqxYEZdeemncfffdMX/+/HjwwQfj6aefjq9//evH9p4AAONG3oGyaNGi2Lt3b6xcuTI6Oztj5syZsXHjxtwHYXft2hWFhe+cmJk3b1488MADccstt8QXv/jF+MAHPhAPP/xwXHjhhSNefElJSTQ3Nx/ydhBHzx6OjP0bOXs4MvZv5OzhyIzW/uX9OygAAKPN3+IBAJIjUACA5AgUACA5AgUASM4JFyj79++Pq666KsrKymLChAlxzTXXxBtvvHHEOZdddlkUFBQMutxwww3HacVjr7W1Naqrq6O0tDRqampiy5YtRxz/3e9+N84///woLS2Niy66KDZs2HCcVpqmfPbv/vvvP+SxVlpaehxXm5bHH388FixYEFOnTo2CgoIj/g2ut23evDk++tGPRklJSZx33nlx//33j/o6U5bvHm7evPmQx2BBQcER/7zIeNbS0hIXX3xxnHnmmTF58uRYuHBhvPDCC+85z/PgW4azf8fqefCEC5Srrroqnnvuudi0aVP88Ic/jMcffzyuv/7695x33XXXxeuvv567fOUrXzkOqx1769ati4aGhmhubo6tW7fGjBkzor6+Pvbs2XPY8U8++WQsXrw4rrnmmti2bVssXLgwFi5cGM8+++xxXnka8t2/iLd+TfF3H2uvvvrqcVxxWnp7e2PGjBnR2tp6VONffvnlmD9/flx++eWxffv2uOmmm+Laa6+NRx55ZJRXmq589/BtL7zwwqDH4eTJk0dphWn78Y9/HMuXL4+f/vSnsWnTpvjNb34TV1xxRfT29g45x/PgO4azfxHH6HkwO4H87Gc/yyIi+6//+q/csf/8z//MCgoKstdee23IeZdeemm2YsWK47DC9MydOzdbvnx57t/9/f3Z1KlTs5aWlsOO//M///Ns/vz5g47V1NRkf/3Xfz2q60xVvvv3zW9+MysvLz9OqzuxRET20EMPHXHM5z//+ezDH/7woGOLFi3K6uvrR3FlJ46j2cPHHnssi4jsf//3f4/Lmk40e/bsySIi+/GPfzzkGM+DQzua/TtWz4Mn1BmU9vb2mDBhQsyZMyd3rK6uLgoLC+Opp5464tx/+7d/i0mTJsWFF14YTU1N8eab4//Pi/f19UVHR0fU1dXljhUWFkZdXV20t7cfdk57e/ug8RER9fX1Q44fz4azfxERb7zxRvzBH/xBVFVVxZ/+6Z/Gc889dzyWOy54/B07M2fOjClTpsQnP/nJ+MlPfjLWy0lGd3d3RERMnDhxyDEeh0M7mv2LODbPgydUoHR2dh5ymvKUU06JiRMnHvH91SuvvDK+/e1vx2OPPRZNTU3xrW99K/7yL/9ytJc75vbt2xf9/f25X/l9W0VFxZD71dnZmdf48Ww4+zd9+vRYu3Zt/OAHP4hvf/vbMTAwEPPmzYtf/OIXx2PJJ7yhHn89PT3x61//eoxWdWKZMmVKrFmzJr7//e/H97///aiqqorLLrsstm7dOtZLG3MDAwNx0003xSWXXHLEXzP3PHh4R7t/x+p5MO+fuh8NjY2Ncddddx1xzI4dO4Z9/b/7GZWLLroopkyZEp/4xCfixRdfjHPPPXfY1wvvVltbO+gPYc6bNy8uuOCC+Od//ue4/fbbx3BlnCymT58e06dPz/173rx58eKLL8Y999wT3/rWt8ZwZWNv+fLl8eyzz8YTTzwx1ks5IR3t/h2r58EkAuVv/uZv4q/+6q+OOOacc86JysrKQz6c+Nvf/jb2798flZWVR317NTU1ERGxc+fOcR0okyZNiqKioujq6hp0vKura8j9qqyszGv8eDac/Xu3U089NWbNmhU7d+4cjSWOO0M9/srKyuK0004bo1Wd+ObOnXvSvyh/5jOfyX2x4v3vf/8Rx3oePFQ++/duw30eTOItnve9731x/vnnH/FSXFwctbW18atf/So6Ojpycx999NEYGBjIRcfR2L59e0S8dSp0PCsuLo7Zs2dHW1tb7tjAwEC0tbUNqtvfVVtbO2h8RMSmTZuGHD+eDWf/3q2/vz+eeeaZcf9YO1Y8/kbH9u3bT9rHYJZl8ZnPfCYeeuihePTRR+Pss89+zzkeh+8Yzv6927CfB0f8Mdvj7I//+I+zWbNmZU899VT2xBNPZB/4wAeyxYsX5/7/F7/4RTZ9+vTsqaeeyrIsy3bu3Jl9+ctfzp5++uns5Zdfzn7wgx9k55xzTvbxj398rO7CcfXggw9mJSUl2f3335/97Gc/y66//vpswoQJWWdnZ5ZlWXb11VdnjY2NufE/+clPslNOOSX76le/mu3YsSNrbm7OTj311OyZZ54Zq7swpvLdv9tuuy175JFHshdffDHr6OjI/uIv/iIrLS3NnnvuubG6C2PqwIED2bZt27Jt27ZlEZGtWrUq27ZtW/bqq69mWZZljY2N2dVXX50b/9JLL2Wnn3569rnPfS7bsWNH1tramhUVFWUbN24cq7sw5vLdw3vuuSd7+OGHs//+7//OnnnmmWzFihVZYWFh9qMf/Wis7sKYuvHGG7Py8vJs8+bN2euvv567vPnmm7kxngeHNpz9O1bPgydcoPzP//xPtnjx4uyMM87IysrKsmXLlmUHDhzI/f/LL7+cRUT22GOPZVmWZbt27co+/vGPZxMnTsxKSkqy8847L/vc5z6XdXd3j9E9OP7+8R//Mfv93//9rLi4OJs7d27205/+NPd/l156abZ06dJB47/zne9kH/zgB7Pi4uLswx/+cLZ+/frjvOK05LN/N910U25sRUVF9ulPfzrbunXrGKw6DW9/5fXdl7f3bOnSpdmll156yJyZM2dmxcXF2TnnnJN985vfPO7rTkm+e3jXXXdl5557blZaWppNnDgxu+yyy7JHH310bBafgMPtXUQMelx5HhzacPbvWD0PFvz/BQAAJCOJz6AAAPwugQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcv4fFCSdoYOq9/cAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label of y_test[10]\n",
    "print('Label of X_test[10]: ', y_test[10])\n",
    "#update model and show histogram with X_test[10]:\n",
    "\n",
    "print('Our histogram after update X_test[10]: ')\n",
    "model_1._predict(X_test[10],plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Đánh giá hiệu suất mô hình Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of your Gaussian Naive Bayes model: 0.96\n"
     ]
    }
   ],
   "source": [
    "pred=model_1.predict(X_test)\n",
    "print('Accuracy of your Gaussian Naive Bayes model:', accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Một số câu hỏi lí thuyết"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Đây là câu hỏi về lý thuyết có thể giúp các bạn buổi sung kiến thức cho các buổi phỏng vấn. Các bạn có thể tham khảo bất kì nguồn nào trừ ChatGPT và có thể thêm hình ảnh minh họa để có câu trả lời rõ ràng và dễ hiểu nhất. Và khi đã tham khảo phải liệt kê tài liệu tham khảo? Trường hợp phát hiện đạo văn bài làm sẽ nhận 0 điểm ngay lập tức.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Trường hợp sử dụng Gaussian Naive Bayes? Nêu chí tiết và cho 3 ứng dụng cụ thể (nêu rõ: input là gì? output là gì? mục tiêu sử dụng?)?\n",
    "\n",
    "Câu trả lời:\n",
    "\n",
    "Gaussian Naive Bayes là một phiên bản của thuật toán Naive Bayes, nó được sử dụng khi các thuộc tính (features) đầu vào là **liên tục** và được giả định tuân theo phân phối Gaussian (phân phối chuẩn).\n",
    "\n",
    "**Chi tiết về cách hoạt động:**\n",
    "\n",
    "*   **Giả định:** Các thuộc tính là độc lập với nhau khi cho một lớp (class) cụ thể. Điều này thường là một giả định \"ngây thơ\" (naive), nhưng thường hoạt động tốt trong thực tế.\n",
    "*   **Phân phối Gaussian:** Với mỗi thuộc tính và mỗi lớp, Gaussian Naive Bayes ước tính các tham số của phân phối Gaussian:\n",
    "    *   **Trung bình (mean):** Trung bình của các giá trị thuộc tính trong lớp đó.\n",
    "    *   **Phương sai (variance):** Phương sai của các giá trị thuộc tính trong lớp đó.\n",
    "*   **Tính xác suất:** Khi có một mẫu mới, thuật toán sẽ tính xác suất mẫu đó thuộc về mỗi lớp dựa trên phân phối Gaussian của từng thuộc tính và áp dụng định lý Bayes để tính xác suất hậu nghiệm (posterior probability).\n",
    "*   **Phân loại:** Mẫu sẽ được gán cho lớp có xác suất hậu nghiệm cao nhất.\n",
    "\n",
    "**3 Ứng dụng cụ thể:**\n",
    "\n",
    "| Ứng dụng | Input | Output | Mục tiêu sử dụng |\n",
    "| -------- | ----- | ------ | ---------------- |\n",
    "| **1. Phân loại email spam/không spam** | Các đặc trưng của email (ví dụ: độ dài email, số từ, tần suất các từ nhất định) | Nhãn phân loại: \"spam\" hoặc \"không spam\" | Phân loại email tự động, lọc email rác, bảo vệ hộp thư người dùng. |\n",
    "| **2. Phân loại bệnh dựa trên kết quả xét nghiệm** | Kết quả các xét nghiệm máu (ví dụ: nồng độ đường, cholesterol, tế bào máu)   | Nhãn phân loại: bệnh (hoặc không bệnh) hoặc các loại bệnh cụ thể  | Hỗ trợ bác sĩ trong việc chẩn đoán bệnh, phân loại bệnh nhân, hỗ trợ quyết định điều trị                               |\n",
    "| **3. Phân loại người dùng dựa trên hoạt động web**| Các hành vi của người dùng trên web (ví dụ: thời gian truy cập, trang đã xem, sản phẩm đã mua)| Nhãn phân loại: Các nhóm người dùng (ví dụ: khách hàng tiềm năng, khách hàng trung thành) | Phân loại người dùng, cá nhân hóa trải nghiệm, gợi ý sản phẩm phù hợp, nhắm mục tiêu quảng cáo, cải thiện dịch vụ. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Có điều kiện bắt buộc nào của dữ liệu đầu vào khi sử dụng Gaussian Naive Bayes không?\n",
    "\n",
    "Câu trả lời:\n",
    "\n",
    "Có một số điều kiện quan trọng về dữ liệu đầu vào khi sử dụng Gaussian Naive Bayes:\n",
    "\n",
    "1.  **Thuộc tính liên tục:** Dữ liệu đầu vào phải là các thuộc tính liên tục (ví dụ: chiều cao, cân nặng, nhiệt độ, giá trị xét nghiệm). Gaussian Naive Bayes không phù hợp với thuộc tính rời rạc (ví dụ: màu sắc, giới tính). Nếu dữ liệu có thuộc tính rời rạc, nên cân nhắc các biến thể khác của Naive Bayes như Multinomial Naive Bayes hoặc Bernoulli Naive Bayes.\n",
    "2.  **Phân phối Gaussian (phân phối chuẩn):** Thuật toán giả định rằng mỗi thuộc tính (feature) theo một phân phối Gaussian trong mỗi lớp. Nếu thuộc tính không tuân theo phân phối này (hoặc phân phối khác biệt quá nhiều), hiệu suất có thể giảm. Tuy nhiên, trong thực tế, thuật toán vẫn hoạt động tốt ngay cả khi giả định này không hoàn toàn đúng.\n",
    "3.  **Tính độc lập:** Gaussian Naive Bayes giả định rằng các thuộc tính là độc lập với nhau khi cho một lớp cụ thể. Đây là một giả định \"ngây thơ\" và thường không đúng trong thực tế. Tuy nhiên, thuật toán thường vẫn mang lại kết quả khá tốt ngay cả khi giả định này bị vi phạm.\n",
    "4.  **Không có giá trị thiếu:** Gaussian Naive Bayes hoạt động tốt nhất khi dữ liệu không có giá trị thiếu (NaN). Cần xử lý giá trị thiếu trước khi sử dụng thuật toán (ví dụ: thay thế bằng giá trị trung bình hoặc loại bỏ mẫu).\n",
    "5. **Dữ liệu số:** Dữ liệu đầu vào phải là dữ liệu số, không được có dữ liệu dạng chữ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Tại sao Naive Bayes thường được sử dụng trong các bài toán phân loại văn bản (Text Classification)?\n",
    "Câu trả lời :\n",
    "\n",
    "Naive Bayes là một thuật toán phân loại văn bản thường được sử dụng trong các bài toán phân loại văn bản (Text Classification) vì:\n",
    "\n",
    "1.  **Nhanh và đơn giản:** Naive Bayes là một thuật toán đơn giản và dễ cài đặt. Nó có tốc độ huấn luyện nhanh chóng, đặc biệt là với dữ liệu lớn.\n",
    "2.  **Hiệu quả với dữ liệu nhiều chiều:** Các bài toán phân loại văn bản thường có số lượng thuộc tính (từ vựng) rất lớn. Mặc dù có giả định hạn chế về tính độc lập, Naive Bayes vẫn hoạt động tốt với dữ liệu nhiều chiều như vậy.\n",
    "3.  **Xử lý tốt dữ liệu rời rạc:** Các từ trong văn bản là các thuộc tính rời rạc (dạng \"túi từ - bag of words\", hoặc TF-IDF), và Multinomial Naive Bayes là một biến thể của Naive Bayes được thiết kế đặc biệt để xử lý các thuộc tính rời rạc dạng này.\n",
    "4.  **Hoạt động tốt với ít dữ liệu:** Naive Bayes có thể hoạt động tốt ngay cả khi lượng dữ liệu huấn luyện không quá lớn, nhờ giả định độc lập của nó.\n",
    "\n",
    "**Tuy nhiên:**\n",
    "\n",
    "*   Giả định về tính độc lập là một hạn chế lớn của Naive Bayes. Các từ trong văn bản thường không độc lập với nhau (ví dụ, các cụm từ hoặc các từ có liên quan ngữ nghĩa). Tuy nhiên, dù có hạn chế này, Naive Bayes vẫn cho kết quả tốt trong nhiều trường hợp thực tế.\n",
    "*   Cần tiền xử lý dữ liệu văn bản (ví dụ: loại bỏ stop words, stem/lemmatize từ) để có kết quả tốt nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tài liệu tham khảo:\n",
    "*   Scikit-learn documentation. (n.d.). *sklearn.naive_bayes.GaussianNB*. Retrieved from [https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tài liệu tham khảo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Géron, A. (2019). *Hands-on machine learning with Scikit-Learn, Keras & TensorFlow: Concepts, tools, and techniques to build intelligent systems*. O'Reilly Media.\n",
    "*   Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer Science & Business Media.\n",
    "*   Scikit-learn documentation. (n.d.). *sklearn.naive_bayes.GaussianNB*. Retrieved from [https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "*   w3schools. (n.d.). *Python Machine Learning - Decision Tree*. Retrieved from [https://www.w3schools.com/python/python_ml_decision_tree.asp](https://www.w3schools.com/python/python_ml_decision_tree.asp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
